{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2550c0-2fd7-4132-a5bc-bdf9a7a17012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, TaskType, PeftModel\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "callbacks = EarlyStoppingCallback(early_stopping_patience=3,\n",
    "                                 early_stopping_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b119c0-f7ce-4085-947a-cf42490413bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10783b44-5615-41b6-931c-03beb87ad075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"json\", data_files=\"Physics_questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65514576-6f6b-4deb-a268-bc45146bab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26e75f-d7d9-4c20-bc93-be69054ff54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb4e35-1e15-4c58-81ba-73e3d83dbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#device_map = {\"\": 0}\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config = True \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llama = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                              quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                               low_cpu_mem_usage=True,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                attn_implementation=attn_implementation\n",
    "                                           )\n",
    "if not use_quantization_config:\n",
    "    llama.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50195eb5-9bf3-49ac-9761-8624c535eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efcf72-b72b-4cae-a250-6a25dfd73628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcq(example):\n",
    "    input_text = f\"\"\"Context: {example['input_text']}\n",
    "\n",
    "Question: {example['question']}\n",
    "\n",
    "Options:\n",
    "A: {example['options']['A']}\n",
    "B: {example['options']['B']}\n",
    "C: {example['options']['C']}\n",
    "D: {example['options']['D']}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    target = f\"{example['correct_option']}\"\n",
    "    explanation = example.get('explanation')\n",
    "    if explanation:\n",
    "        target = f\"Answer: {example['correct_option']}\\nExplanation: {explanation}\"\n",
    "\n",
    "    return {'input': input_text, 'target': target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38cf22-fd86-4d44-a439-e649deff6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = [format_mcq(ex) for ex in df[\"train\"]]\n",
    "dataset = Dataset.from_list(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd08a4-b34d-4e01-9967-d44a07daa5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a3e8d-5048-4495-91e3-9ce8092c583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    model_input = tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    label = tokenizer(example[\"target\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_input[\"labels\"] = label[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c4835-89d5-47b5-9846-ca3096d30f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826227e5-9089-4abd-9832-2a7db0e00a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_conf =  LoraConfig(task_type=\"CAUSAL_LM\",\n",
    "                       r=64,\n",
    "                       lora_alpha=16,\n",
    "                       lora_dropout=0.1,\n",
    "                        bias='none',\n",
    "                       target_modules=[\"q_proj\", \"v_proj\"]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440fe8e-ab73-4b20-8bfb-ea2c625c9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Add adapters ===\n",
    "llama = get_peft_model(llama, lora_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57737e-d8f8-46d2-8cb3-3794228828fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4d58a-c91a-4b28-a813-8188fd11f6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9454f-f1c3-4940-80fc-38c353b63237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6b4a6-c01c-42d0-9938-ef19517e550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90566b6e-d34b-4912-a49f-f5c5f1cb0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = TrainingArguments(\n",
    "    output_dir=\"./nairs-sample-3\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=10,\n",
    "    #dataset_text_field= \"input\",\n",
    "    fp16=False,  # or True, depending on your needs\n",
    "    bf16=False,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    #max_seq_length=1042,\n",
    "    #eval_strategy=\"no\",\n",
    "    eval_steps=312,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./Epochs_2\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    #load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cffa54-5f52-407e-a231-57ff11f58ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1b17d-548f-4a8d-ad84-a835ec06cc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e12e8-4f76-4c1c-a922-cc1d9c49c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ebdfd-c64e-4344-be7c-428352fd3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_columns = ['input_ids', 'labels']\n",
    "tokenized_datasets = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in tokenized_columns])\n",
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98b5ce-55cf-4e6c-82ce-e5987346b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=llama,\n",
    "    args=training,\n",
    "    peft_config= lora_conf,\n",
    "    tokenizer = tokenizer,\n",
    "    max_seq_length=1042,\n",
    "    dataset_text_field=\"text\",\n",
    "    train_dataset= tokenized_datasets\n",
    "    #eval_dataset=test,\n",
    "    #callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf667d1a-2334-428a-97ff-18ad93109e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f8d6e-fe78-40ac-83dc-b973bd3a50dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c02bf9-fe23-4adb-8bb2-1fb33de0f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258d9ae-5239-4f9c-939d-7b7327ca5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./nairs-2e\")\n",
    "tokenizer.save_pretrained(\"./nairs-2e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cb464-829f-463f-ac28-a7005a8451e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa130fc-6564-4a48-a77e-6ea724c0625d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8d2d1-51ff-4592-985d-18cee84a2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================\n",
    "# Loading fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f19cee8-e495-4272-a0db-5349ea69c6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: ./nairs-2e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92c2f2877ba4c8ba3f360c8b66e2952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#device_map = {\"\": 0}\n",
    "model_id = \"./nairs-2e\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config = True \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llama = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                              quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                               low_cpu_mem_usage=True,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                attn_implementation=attn_implementation\n",
    "                                           )\n",
    "if not use_quantization_config:\n",
    "    llama.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160826e8-c99a-41b4-b854-dfa183b7d014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32589f-5482-43c5-b05a-2559eac5fe1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abe5a-25e8-4134-9853-40054a078a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_physics_assessment(nairs, tokenizer, context, max_new_tokens=600, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generates properly formatted physics assessments with guaranteed structure.\n",
    "    Implements multiple fallback mechanisms for reliable output.\n",
    "    \"\"\"\n",
    "    # 1. Create an explicit few-shot prompt with clear formatting examples\n",
    "    prompt = f\"\"\"Generate an assessment question with options and provide a detailed explanation using EXACTLY this format:\n",
    "\n",
    "Example 1:\n",
    "Context: When soldiers march across a suspension bridge...\n",
    "Question: Why are marching soldiers advised to break step on bridges?\n",
    "Options:\n",
    "A: To reduce air resistance\n",
    "B: To prevent resonance\n",
    "C: To minimize friction\n",
    "D: To decrease bridge weight\n",
    "Answer: B\n",
    "Explanation: Marching soldiers are advised to break step on bridges to prevent resonance. When soldiers march in unison, their rhythmic footsteps can match the bridge's natural frequency. This matching of frequencies can cause the bridge to oscillate with increasing amplitude, potentially leading to structural damage. Breaking step ensures that the periodic force isn't applied at the bridge's natural frequency, preventing dangerous resonance effects.\n",
    "\n",
    "Now generate for:\n",
    "Context: {context}\n",
    "Question:\"\"\"\n",
    "\n",
    "    # 2. Generate the output with conservative parameters\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = nairs.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # 3. Extract and clean the generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = full_output.split(\"Question:\")[-1].strip()\n",
    "    return generated_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff08a3-5652-4638-b8b9-67b3e44c4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"We have been thought Temperature but can't understand it\"\n",
    "assessment = generate_physics_assessment(nairs, tokenizer, context)\n",
    "print(assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1d912-14da-4502-b452-aed0628285ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ae4b9-2dc2-4e1e-8427-e24e8dbf2737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06983ef6-adc7-4cd9-800b-f376bb511c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
