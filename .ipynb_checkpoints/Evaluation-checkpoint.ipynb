{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e2550c0-2fd7-4132-a5bc-bdf9a7a17012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, TaskType, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e8bf3-30bf-4233-9486-efebfea464ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a3e55c-9661-4162-8143-7d4a79080f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================\n",
    "#1. Import Test Data\n",
    "#===========================\n",
    "with open(\"test_data.json\") as json_data:\n",
    "    df = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca0b079-8ba6-42b7-bd7d-c7bd3d398129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'In quantum mechanics, wave-particle duality describes how particles exhibit both wave-like and particle-like properties. This principle is fundamental to understanding phenomena like electron diffraction.',\n",
       " 'question': 'Which experiment first demonstrated the wave nature of electrons?',\n",
       " 'options': {'A': 'Michelson-Morley experiment',\n",
       "  'B': 'Davisson-Germer experiment',\n",
       "  'C': \"Young's double-slit experiment with light\",\n",
       "  'D': \"Rutherford's gold foil experiment\"},\n",
       " 'correct_option': 'B',\n",
       " 'explanation': 'The Davisson-Germer experiment (1927) confirmed the wave nature of electrons by observing diffraction patterns when electrons were scattered by a nickel crystal.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5facc-f200-47aa-80c1-74db9efab5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6957d-c532-43a1-a598-f8a02e325e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b41e6ae-f475-42de-b58d-a71bc87ece6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================\n",
    "# 2. Import fine-tuned model\n",
    "#==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e832c9a-9c6a-4015-b914-c34b98abe387",
   "metadata": {},
   "source": [
    "## Merging the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7cbfd8-dea4-43c0-86b4-0b250e69dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: ./nairs-2e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a40c4a45af74479850e94442c176205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#device_map = {\"\": 0}\n",
    "model_id = \"./nairs-2e\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config = True \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "nairs = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                              quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                               low_cpu_mem_usage=True,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                attn_implementation=attn_implementation\n",
    "                                           )\n",
    "if not use_quantization_config:\n",
    "    nairs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00c721-328a-4be7-a6f0-47d06e6a7b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1ef003-5c7f-4d54-a1e4-ea3aa97a5bb8",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464db205-bef3-48c2-93ec-3b20157965a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# 3. Zero Shot Prompt Test\n",
    "#================================\n",
    "prompt = \"Input_text: I have a problem understanding Temperature in Physics\"\n",
    "input_ids = tokenizer(prompt, return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02961fd4-63f9-4d13-a795-ae2977c10d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: Input_text: I have a problem understanding Temperature in Physics.\n",
      "Question: Which of these is NOT a measure of temperature?\n",
      "Options:\n",
      "A: Mass\n",
      "B: Density\n",
      "C: Volume\n",
      "D: Energy\n",
      "\n",
      "Question: Options:\n",
      "\n",
      "Answer: D: Energy\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Options A, B, and C are all related to physical properties that are not directly related to temperature. Energy, on the other hand, is not a measure of temperature.\n",
      "\n",
      "Options:\n",
      "\n",
      "A: Mass: The heavier the object, the higher its temperature.\n",
      "B: Density: The density of a substance does not affect its temperature.\n",
      "C: Volume: The volume of a substance does not affect its temperature.\n",
      "D: Energy: Energy is not directly related to temperature.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The SI unit of temperature is Kelvin (K), which is based on the absolute scale. The temperature of absolute zero is 0 K.\n",
      "\n",
      "Options:\n",
      "\n",
      "A: Mass: The heavier the object, the higher its temperature.\n",
      "B: Density: The density of a substance does not affect its temperature.\n",
      "C: Volume: The volume of a substance does not affect its temperature.\n",
      "D: Energy: Energy is not directly related to temperature.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The SI unit of temperature is Kelvin (\n"
     ]
    }
   ],
   "source": [
    "outputs = nairs.generate(\n",
    "    **input_ids, \n",
    "    max_length=300,\n",
    "    temperature=0.7,\n",
    "    do_sample = True\n",
    ")\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Results:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0565ab-3dfb-457d-bcb9-ccc41aa2b83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6998b-12be-4911-9598-961611e2dff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db336f-9950-4148-9ef5-a7b5bceea559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431a5327-e26a-4f2a-bbc7-db47b493f675",
   "metadata": {},
   "source": [
    "## Few shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ffaae0-bd45-41d1-b9a1-c950f9835b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# 4. Few Shot Prompt Test\n",
    "#================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d59abe5a-25e8-4134-9853-40054a078a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_physics_assessment(nairs, tokenizer, context, max_new_tokens=300, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generates properly formatted physics assessments with guaranteed structure.\n",
    "    Implements multiple fallback mechanisms for reliable output.\n",
    "    \"\"\"\n",
    "    # 1. Create an explicit few-shot prompt with clear formatting examples\n",
    "    prompt = f\"\"\"Generate an assessment question with options and provide a detailed explanation using EXACTLY this format:\n",
    "\n",
    "Example 1:\n",
    "Context: When soldiers march across a suspension bridge...\n",
    "Question: Why are marching soldiers advised to break step on bridges?\n",
    "Options:\n",
    "A: To reduce air resistance\n",
    "B: To prevent resonance\n",
    "C: To minimize friction\n",
    "D: To decrease bridge weight\n",
    "Answer: B\n",
    "Explanation: Marching soldiers are advised to break step on bridges to prevent resonance. When soldiers march in unison, their rhythmic footsteps can match the bridge's natural frequency. This matching of frequencies can cause the bridge to oscillate with increasing amplitude, potentially leading to structural damage. Breaking step ensures that the periodic force isn't applied at the bridge's natural frequency, preventing dangerous resonance effects.\n",
    "\n",
    "Now generate for:\n",
    "Context: {context}\n",
    "Question:\"\"\"\n",
    "\n",
    "    # 2. Generate the output with conservative parameters\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = nairs.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # 3. Extract and clean the generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = full_output.split(\"Question:\")[-1].strip()\n",
    "    return generated_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58ff08a3-5652-4638-b8b9-67b3e44c4573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the SI unit of temperature?\n",
      "Options:\n",
      "A: Celsius\n",
      "B: Kelvin\n",
      "C: Fahrenheit\n",
      "D: Joule\n",
      "Answer: B\n",
      "Explanation: The SI unit of temperature is the Kelvin, denoted by the symbol K. One degree Celsius is equal to one degree Kelvin, and 0°C = 273.15°K.\n",
      "\n",
      "Please answer in the format provided.\n"
     ]
    }
   ],
   "source": [
    "context = \"I have a problem understanding Temperature in Physics\"\n",
    "assessment = generate_physics_assessment(nairs, tokenizer, context)\n",
    "print(assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a0549-e06e-4e6a-9c2a-01de1981b76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282dbc04-70da-49d9-8c60-22132a12b786",
   "metadata": {},
   "source": [
    "## Evaluating fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e67c4e-decd-496b-b74f-1b21b2d8d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================\n",
    "# 5. Zero shot\n",
    "#========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "701288b6-939d-44e4-bdaa-f37b335cb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(nairs, tokenizer, df):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for item in df:\n",
    "        # Format the question and options\n",
    "        input_text = (\n",
    "            f\"Context: {item['input_text']}\\n\"\n",
    "            f\"Question: {item['question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A) {item['options']['A']}\\n\"\n",
    "            f\"B) {item['options']['B']}\\n\"\n",
    "            f\"C) {item['options']['C']}\\n\"\n",
    "            f\"D) {item['options']['D']}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        outputs = nairs.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.8, \n",
    "        )\n",
    "\n",
    "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]  # Takes last character\n",
    "\n",
    "        if predicted_answer == item[\"correct_option\"]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94ccb5c6-55bd-4c26-aa30-2b04c7207fd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def evaluate_accuracy(nairs, tokenizer, df):\\n    correct = 0\\n    total = 0\\n\\n    for item in df:\\n        # Format the question and options\\n        input_text = (\\n            f\"Context: {item[\\'input_text\\']}\\n\"\\n            f\"Question: {item[\\'question\\']}\\n\"\\n            \"Options:\\n\"\\n            f\"A) {item[\\'options\\'][\\'A\\']}\\n\"\\n            f\"B) {item[\\'options\\'][\\'B\\']}\\n\"\\n            f\"C) {item[\\'options\\'][\\'C\\']}\\n\"\\n            f\"D) {item[\\'options\\'][\\'D\\']}\\n\"\\n            \"Answer:\"\\n        )\\n\\n        \\n        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\\n\\n        \\n        outputs = nairs.generate(\\n            **inputs,\\n            max_new_tokens=2,\\n            temperature=0.7,  \\n            do_sample=False,\\n            pad_token_id=tokenizer.eos_token_id,\\n        )\\n\\n        \\n        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]  # Takes last character\\n\\n        # Check if correct\\n        if predicted_answer == item[\"correct_option\"]:\\n            correct += 1\\n        total += 1\\n\\n    accuracy = correct / total if total > 0 else 0.0\\n    return accuracy'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def evaluate_accuracy(nairs, tokenizer, df):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for item in df:\n",
    "        # Format the question and options\n",
    "        input_text = (\n",
    "            f\"Context: {item['input_text']}\\n\"\n",
    "            f\"Question: {item['question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A) {item['options']['A']}\\n\"\n",
    "            f\"B) {item['options']['B']}\\n\"\n",
    "            f\"C) {item['options']['C']}\\n\"\n",
    "            f\"D) {item['options']['D']}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "\n",
    "        \n",
    "        outputs = nairs.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.7,  \n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        \n",
    "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]  # Takes last character\n",
    "\n",
    "        # Check if correct\n",
    "        if predicted_answer == item[\"correct_option\"]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "018773bf-898f-4f7d-a0a9-1ec5ae142e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = evaluate_accuracy(nairs, tokenizer, df)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54ce8931-763a-4ca4-87a9-b45e6f3dd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1 ---\n",
      "Context: In quantum mechanics, wave-particle duality describes how particles exhibit both wave-like and particle-like properties. This principle is fundamental to understanding phenomena like electron diffraction.\n",
      "Question: Which experiment first demonstrated the wave nature of electrons?\n",
      "Options:\n",
      "A) Michelson-Morley experiment\n",
      "B) Davisson-Germer experiment\n",
      "C) Young's double-slit experiment with light\n",
      "D) Rutherford's gold foil experiment\n",
      "Model's Answer: :\n",
      "Correct Answer: B\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "--- Question 2 ---\n",
      "Context: Special relativity introduces the concept of time dilation, where time intervals measured in a moving frame appear longer to a stationary observer.\n",
      "Question: A clock traveling at 0.8c relative to Earth will appear to run slower by what factor compared to Earth's clock?\n",
      "Options:\n",
      "A) 0.6\n",
      "B) 0.8\n",
      "C) 1.25\n",
      "D) 1.67\n",
      "Model's Answer: :\n",
      "Correct Answer: A\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "--- Question 3 ---\n",
      "Context: Thermodynamics states that entropy in an isolated system never decreases, as described by the Second Law.\n",
      "Question: Which process violates the Second Law of Thermodynamics?\n",
      "Options:\n",
      "A) Heat flowing from hot to cold\n",
      "B) Spontaneous decrease in entropy\n",
      "C) Energy conservation in a closed system\n",
      "D) Isothermal expansion of a gas\n",
      "Model's Answer: B\n",
      "Correct Answer: B\n",
      "Result: ✅ Correct\n",
      "\n",
      "--- Question 4 ---\n",
      "Context: In electromagnetism, Maxwell's equations unify electric and magnetic fields and predict electromagnetic waves.\n",
      "Question: Which Maxwell equation describes how a changing magnetic field induces an electric field?\n",
      "Options:\n",
      "A) Gauss's Law\n",
      "B) Faraday's Law\n",
      "C) Ampere's Law\n",
      "D) Gauss's Law for Magnetism\n",
      "Model's Answer: :\n",
      "Correct Answer: B\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "--- Question 5 ---\n",
      "Context: Nuclear physics explains binding energy as the energy required to disassemble a nucleus into its constituent protons and neutrons.\n",
      "Question: Where does the maximum binding energy per nucleon occur on the binding energy curve?\n",
      "Options:\n",
      "A) Hydrogen (A=1)\n",
      "B) Iron (A=56)\n",
      "C) Uranium (A=238)\n",
      "D) Helium (A=4)\n",
      "Model's Answer: B\n",
      "Correct Answer: B\n",
      "Result: ✅ Correct\n",
      "\n",
      "--- Question 6 ---\n",
      "Context: In particle physics, the Standard Model classifies fundamental particles into quarks, leptons, and gauge bosons.\n",
      "Question: Which particle mediates the electromagnetic force?\n",
      "Options:\n",
      "A) Gluon\n",
      "B) W boson\n",
      "C) Photon\n",
      "D) Higgs boson\n",
      "Model's Answer: )\n",
      "Correct Answer: C\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "--- Question 7 ---\n",
      "Context: General relativity describes gravity as the curvature of spacetime caused by mass and energy.\n",
      "Question: What observable phenomenon confirmed Einstein's theory of general relativity in 1919?\n",
      "Options:\n",
      "A) Gravitational redshift\n",
      "B) Time dilation in GPS satellites\n",
      "C) Bending of starlight by the Sun\n",
      "D) Black hole mergers\n",
      "Model's Answer: A\n",
      "Correct Answer: C\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "--- Question 8 ---\n",
      "Context: In condensed matter physics, superconductors exhibit zero electrical resistance below a critical temperature.\n",
      "Question: What is the BCS theory used to explain?\n",
      "Options:\n",
      "A) High-temperature superconductivity\n",
      "B) Formation of Cooper pairs\n",
      "C) Quantum Hall effect\n",
      "D) Semiconductor band gaps\n",
      "Model's Answer: B\n",
      "Correct Answer: B\n",
      "Result: ✅ Correct\n",
      "\n",
      "--- Question 9 ---\n",
      "Context: Quantum field theory combines quantum mechanics with special relativity to describe particle interactions.\n",
      "Question: Which quantum field theory describes the strong nuclear force?\n",
      "Options:\n",
      "A) Electrodynamics (QED)\n",
      "B) Chromodynamics (QCD)\n",
      "C) Electroweak Theory\n",
      "D) String Theory\n",
      "Model's Answer: B\n",
      "Correct Answer: B\n",
      "Result: ✅ Correct\n",
      "\n",
      "--- Question 10 ---\n",
      "Context: The Heisenberg Uncertainty Principle states that certain pairs of physical properties cannot be simultaneously measured with arbitrary precision.\n",
      "Question: Which pair of properties is governed by the Uncertainty Principle?\n",
      "Options:\n",
      "A) Energy and time\n",
      "B) Charge and mass\n",
      "C) Velocity and acceleration\n",
      "D) Temperature and entropy\n",
      "Model's Answer: C\n",
      "Correct Answer: A\n",
      "Result: ❌ Incorrect\n",
      "\n",
      "Final Accuracy: 40.00% (4/10)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, tokenizer, test_data, verbose=True):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, item in enumerate(test_data, 1):\n",
    "        # Format the input\n",
    "        input_text = (\n",
    "            f\"Context: {item['input_text']}\\n\"\n",
    "            f\"Question: {item['question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A) {item['options']['A']}\\n\"\n",
    "            f\"B) {item['options']['B']}\\n\"\n",
    "            f\"C) {item['options']['C']}\\n\"\n",
    "            f\"D) {item['options']['D']}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode and extract the predicted answer\n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_answer = full_output.strip()[-1]  # Takes last character (A/B/C/D)\n",
    "\n",
    "        # Check correctness\n",
    "        is_correct = predicted_answer == item[\"correct_option\"]\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        # Print detailed results if verbose=True\n",
    "        if verbose:\n",
    "            print(f\"\\n--- Question {idx} ---\")\n",
    "            print(f\"Context: {item['input_text']}\")\n",
    "            print(f\"Question: {item['question']}\")\n",
    "            print(\"Options:\")\n",
    "            for opt, desc in item['options'].items():\n",
    "                print(f\"{opt}) {desc}\")\n",
    "            print(f\"Model's Answer: {predicted_answer}\")\n",
    "            print(f\"Correct Answer: {item['correct_option']}\")\n",
    "            print(f\"Result: {'✅ Correct' if is_correct else '❌ Incorrect'}\")\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\nFinal Accuracy: {accuracy * 100:.2f}% ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "# Example Usage:\n",
    "with open(\"test_data.json\") as json_data:\n",
    "    test_data = json.load(json_data)\n",
    "\n",
    "accuracy = evaluate_accuracy(nairs, tokenizer, test_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0931924-2d7b-49f5-b0d0-f22897d0e3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7582b-2640-498e-b572-36ac20afbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================\n",
    "# 6. Few shot Evaluation\n",
    "#========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5444da0-eb0e-424d-baf9-6994bd7c906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input_text\": \"When soldiers march across a suspension bridge...\",\n",
    "        \"question\": \"Why are marching soldiers advised to break step on bridges?\",\n",
    "        \"options\": {\n",
    "            \"A\": \"To reduce air resistance\",\n",
    "            \"B\": \"To prevent resonance\",\n",
    "            \"C\": \"To minimize friction\",\n",
    "            \"D\": \"To decrease bridge weight\"\n",
    "        },\n",
    "        \"correct_option\": \"B\",\n",
    "        \"explanation\": \"Marching soldiers are advised to break step to prevent resonance, which could amplify vibrations and damage the bridge.\"\n",
    "    },\n",
    "    {\n",
    "        \"input_text\": \"In an electric circuit, resistance opposes current flow...\",\n",
    "        \"question\": \"What happens to current if resistance increases while voltage stays constant?\",\n",
    "        \"options\": {\n",
    "            \"A\": \"Current increases\",\n",
    "            \"B\": \"Current decreases\",\n",
    "            \"C\": \"Current remains the same\",\n",
    "            \"D\": \"Voltage must change\"\n",
    "        },\n",
    "        \"correct_option\": \"B\",\n",
    "        \"explanation\": \"According to Ohm's Law (V=IR), if resistance increases and voltage is constant, current must decrease.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e03af6f-258f-42e6-a30b-21fbcafe59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_with_few_shot(nairs, tokenizer, df, few_shot_examples):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Ensure test_data is a list\n",
    "    if isinstance(df, dict):\n",
    "        df = [df]\n",
    "\n",
    "    for item in df:\n",
    "        few_shot_prompt = \"\"\n",
    "        for example in few_shot_examples:\n",
    "            few_shot_prompt += (\n",
    "                f\"Context: {example['input_text']}\\n\"\n",
    "                f\"Question: {example['question']}\\n\"\n",
    "                \"Options:\\n\"\n",
    "                f\"A) {example['options']['A']}\\n\"\n",
    "                f\"B) {example['options']['B']}\\n\"\n",
    "                f\"C) {example['options']['C']}\\n\"\n",
    "                f\"D) {example['options']['D']}\\n\"\n",
    "                f\"Answer: {example['correct_option']}\\n\"\n",
    "                f\"Explanation: {example['explanation']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "        # Final prompt = Few-shot examples + Current question\n",
    "        input_text = (\n",
    "            few_shot_prompt +\n",
    "            f\"Context: {item['input_text']}\\n\"\n",
    "            f\"Question: {item['question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A) {item['options']['A']}\\n\"\n",
    "            f\"B) {item['options']['B']}\\n\"\n",
    "            f\"C) {item['options']['C']}\\n\"\n",
    "            f\"D) {item['options']['D']}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        outputs = nairs.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.8,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Extract predicted answer (last character)\n",
    "        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()[-1]\n",
    "\n",
    "        # Check correctness\n",
    "        if predicted_answer == item[\"correct_option\"]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "001acf6b-e6cc-4f7b-ac71-eaaf5adb3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_accuracy_with_few_shot(nairs, tokenizer, df, few_shot_examples)\n",
    "print(f\"Few-shot Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979e808-991b-49b6-a829-dc39a961b4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d8eab7-04e2-4454-80f1-5f7f266a5843",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d06687-89ca-4da8-898e-69dfd672a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b8053fa-b591-4183-bf8f-de7d021ec02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_few_shot(context, question, options, few_shot_examples=True):\n",
    "    prompt = \"\"\n",
    "\n",
    "    if few_shot_examples:\n",
    "        for example in few_shot_examples:\n",
    "            prompt += (\n",
    "                f\"Context: {example['input_text']}\\n\"\n",
    "                f\"Question: {example['question']}\\n\"\n",
    "                \"Options:\\n\"\n",
    "                f\"A) {example['options']['A']}\\n\"\n",
    "                f\"B) {example['options']['B']}\\n\"\n",
    "                f\"C) {example['options']['C']}\\n\"\n",
    "                f\"D) {example['options']['D']}\\n\"\n",
    "                f\"Answer: {example['correct_option']}\\n\"\n",
    "                f\"Explanation: {example['explanation']}\\n\\n\"\n",
    "            )\n",
    "    \n",
    "    # Add the current question\n",
    "    prompt += (\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Options:\\n\"\n",
    "        f\"A) {options['A']}\\n\"\n",
    "        f\"B) {options['B']}\\n\"\n",
    "        f\"C) {options['C']}\\n\"\n",
    "        f\"D) {options['D']}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9245a21f-376a-4b42-a108-222dd08178cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(full_output):\n",
    "    match = re.search(r\"Answer:\\s*([A-D])\", full_output, re.IGNORECASE)\n",
    "    return match.group(1).strip().upper() if match else None\n",
    "def extract_explanation(full_output):\n",
    "    match = re.search(r\"Explanation:\\s*(.+)\", full_output, re.IGNORECASE | re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"No explanation generated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a8068-e824-465d-98be-59a325abcf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27dc072d-4de1-45c7-a1e8-adc16e96d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# 7. Rouge Evaluation -- Few-shot\n",
    "#================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca7c6658-a8c3-4ade-9a7a-16a3ea194785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_and_explanation(nairs, tokenizer, context, question, options, few_shot_examples=True):\n",
    "    prompt = build_prompt_with_few_shot(context, question, options, few_shot_examples)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    outputs = nairs.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.8,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract full generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse answer (e.g., \"Answer: B\") and explanation\n",
    "    answer = extract_answer(full_output)  # e.g., \"B\"\n",
    "    explanation = extract_explanation(full_output)  # e.g., \"The Davisson-Germer experiment...\"\n",
    "    \n",
    "    return answer, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbdeaebf-d71d-43d2-8c26-7d3b0983720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_scores(generated_explanations, reference_explanations):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {\n",
    "        'rouge1': {'precision': [], 'recall': [], 'f1': []},\n",
    "        'rouge2': {'precision': [], 'recall': [], 'f1': []},\n",
    "        'rougeL': {'precision': [], 'recall': [], 'f1': []}\n",
    "    }\n",
    "    \n",
    "    for gen, ref in zip(generated_explanations, reference_explanations):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key]['precision'].append(scores[key].precision)\n",
    "            rouge_scores[key]['recall'].append(scores[key].recall)\n",
    "            rouge_scores[key]['f1'].append(scores[key].fmeasure)\n",
    "    \n",
    "    # Compute averages\n",
    "    avg_scores = {\n",
    "        'rouge1': {k: sum(v) / len(v) for k, v in rouge_scores['rouge1'].items()},\n",
    "        'rouge2': {k: sum(v) / len(v) for k, v in rouge_scores['rouge2'].items()},\n",
    "        'rougeL': {k: sum(v) / len(v) for k, v in rouge_scores['rougeL'].items()}\n",
    "    }\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db8ab881-360e-4f08-9420-208e2c778cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': {'precision': 0.05752416448819112, 'recall': 0.699220823798627, 'f1': 0.10610336873518533}, 'rouge2': {'precision': 0.023715994130131472, 'recall': 0.3089960332065595, 'f1': 0.04395255851652705}, 'rougeL': {'precision': 0.041959639348942424, 'recall': 0.5208607388035307, 'f1': 0.07750564963905676}}\n"
     ]
    }
   ],
   "source": [
    "generated_explanations = []\n",
    "reference_explanations = []\n",
    "\n",
    "for item in df:\n",
    "    # Generate model output\n",
    "    _, explanation = generate_answer_and_explanation(\n",
    "        nairs, tokenizer,\n",
    "        context=item[\"input_text\"],\n",
    "        question=item[\"question\"],\n",
    "        options=item[\"options\"],\n",
    "        few_shot_examples=few_shot_examples\n",
    "    )\n",
    "    \n",
    "    generated_explanations.append(explanation)\n",
    "    reference_explanations.append(item[\"explanation\"])\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = compute_rouge_scores(generated_explanations, reference_explanations)\n",
    "print(\"ROUGE Scores:\", rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95944b5-8003-43dd-9dd9-352444364a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
