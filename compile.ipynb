{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e54068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, TaskType, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch_tensorrt\n",
    "import modelopt\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"./nairs-2e\"\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "#                                          bnb_4bit_compute_dtype=torch.float16)\n",
    "# use_quantization_config = True \n",
    "# if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "#   attn_implementation = \"flash_attention_2\"\n",
    "# else:\n",
    "#   attn_implementation = \"sdpa\"\n",
    "# print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "# print(f\"[INFO] Using model_id: {model_id}\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "# llama = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "#                                              torch_dtype = torch.float16,\n",
    "#                                               quantization_config=quantization_config if use_quantization_config else None,\n",
    "#                                                low_cpu_mem_usage=True,\n",
    "#                                                  device_map = \"auto\",\n",
    "#                                                 attn_implementation=attn_implementation,\n",
    "#                                              return_dict = True,\n",
    "#                                              torchscript = True,\n",
    "#                                            )\n",
    "# if not use_quantization_config:\n",
    "#     llama.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ff2fa7-7cfb-471e-8f31-764505a4fd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5422fa77e64f2bb34de031f1ccf676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "model_id = \"./nairs-2d\"\n",
    "peft_config = PeftConfig.from_pretrained(model_id)\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"cpu\"  # load on CPU for now\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786494aa-8de5-4c48-8238-7cf0bc1493f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef476ff-8409-4fe0-9609-d52ee751e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./nairs_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3c87d-499b-4bc6-8472-5a7e87a366eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f3c6ad51bb48d3972af4e270be38a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:15<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"./nairs-merged\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./nairs_merged\",\n",
    "                                 device_map = \"cuda\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c891e-11de-430d-9559-15fa771faaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d317b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: ./nairs_merged\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at './nairs_merged'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/n_project/lib/python3.9/site-packages/peft/config.py:195\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/n_project/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/n_project/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './nairs_merged'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Using attention implementation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_implementation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Using model_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m peft \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft\u001b[38;5;241m.\u001b[39mbase_model_name_or_path,\n\u001b[1;32m     14\u001b[0m                                                 attn_implementation\u001b[38;5;241m=\u001b[39mattn_implementation,\n\u001b[1;32m     15\u001b[0m                                                 quantization_config\u001b[38;5;241m=\u001b[39mquantization_config \u001b[38;5;28;01mif\u001b[39;00m use_quantization_config \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                                                 device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m                                                  )\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model,\n\u001b[1;32m     20\u001b[0m                                   model_id,\n\u001b[1;32m     21\u001b[0m                                  )\n",
      "File \u001b[0;32m~/anaconda3/envs/n_project/lib/python3.9/site-packages/peft/config.py:199\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    196\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    201\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    202\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at './nairs_merged'"
     ]
    }
   ],
   "source": [
    "model_id = \"./nairs_merged\"\n",
    "#base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "use_quantization_config = True \n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "peft = PeftConfig.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft.base_model_name_or_path,\n",
    "                                                attn_implementation=attn_implementation,\n",
    "                                                quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                device_map = \"auto\",\n",
    "\n",
    "                                                 )\n",
    "model = PeftModel.from_pretrained(base_model,\n",
    "                                  model_id,\n",
    "                                 )\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft.base_model_name_or_path)\n",
    "if not use_quantization_config:\n",
    "    model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99413ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa05de6c-dc64-4b4b-ba12-2f53be9afeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create strict wrapper\n",
    "class ScriptableWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=None,\n",
    "            position_ids=None,\n",
    "            past_key_values=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            use_cache=True,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            torchscript=None\n",
    "        )[0]  # Return just logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef962c8b-127f-4cf6-8a5d-50f9686aa547",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = tokenizer(\"What is the principle of Relativity?\", return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "# 4. Script the model\n",
    "wrapped_model = ScriptableWrapper(model).eval()\n",
    "with torch.no_grad():\n",
    "    scripted_model = torch.jit.trace(wrapped_model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6edb5fa-aa6e-4bd4-b399-954326a01ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_settings = {\n",
    "    \"inputs\": [torch_tensorrt.Input(\n",
    "        min_shape=[1, 1],    # Minimum input shape\n",
    "        opt_shape=[1, 128],  # Optimal/typical shape\n",
    "        max_shape=[1, 256], # Maximum shape\n",
    "        dtype=torch.float16\n",
    "    )],\n",
    "    \"enabled_precisions\": {torch.float16},\n",
    "    \"workspace_size\": 1 << 28,  # 256MB workspace instead of default 1GB\n",
    "    \"ir\": \"torchscript\",\n",
    "    \"truncate_long_and_double\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c7d2e-e200-412c-82eb-d29b5363bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nairs = torch_tensorrt.compile(scripted_model, **compile_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71079876-c57f-4983-a792-bf0720546c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/jit/_trace.py:809: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_input = tokenizer(\"What is the principle of Relativity?\", return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "traced_model = torch.jit.trace(scripted_model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c809f-b49e-42fe-988b-5e537ccddabf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def safe_compile(model, compile_settings, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "            \n",
    "            # Gradually reduce workspace size on failures\n",
    "            current_settings = compile_settings.copy()\n",
    "            current_settings[\"workspace_size\"] = 1 << (30 - attempt * 2)  # 1GB → 256MB → 64MB\n",
    "            \n",
    "            print(f\"Attempt {attempt + 1} with workspace {current_settings['workspace_size'] >> 20}MB\")\n",
    "            return torch_tensorrt.compile(model, **current_settings)\n",
    "    \n",
    "# Usage:\n",
    "nairs = safe_compile(\n",
    "    scripted_model,\n",
    "    compile_settings={\n",
    "        \"inputs\": [torch_tensorrt.Input(\n",
    "            min_shape=[1, 1],\n",
    "            opt_shape=[1, 128],\n",
    "            max_shape=[1, 256],\n",
    "            dtype=torch.float16\n",
    "        )],\n",
    "        \"enabled_precisions\": {torch.float16},\n",
    "        \"truncate_long_and_double\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09ee41-71b3-4a33-8546-3d931d2389d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_settings = {\n",
    "    \"inputs\": [\n",
    "        torch_tensorrt.Input(\n",
    "            shape = [1,7],\n",
    "            dtype = torch.float16,\n",
    "            \n",
    "        )\n",
    "    ],\n",
    "    \"enabled_precisions\": {torch.half},\n",
    "    \"ir\": \"torchscript\"\n",
    "}\n",
    "nairs = torch_tensorrt.compile(scripted_model, **compile_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad671b-2989-46c8-86ac-6064682418fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped = ScriptableWrapper(model)\n",
    "example_input = tokenizer(\"What is the principle of Relativity?\", return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d8376-0706-4fb6-b855-8ecdf3c2c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baff89-adb2-406b-a739-47d732ad2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracedModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids=input_ids)\n",
    "        return outputs.logits  # Return only logits\n",
    "\n",
    "wrapped_model = TracedModelWrapper(model)\n",
    "traced_model = torch.jit.trace(wrapped_model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bf953-a1ed-43aa-bf9c-3dbed971288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.script(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccf4d-8bb8-406c-a68d-8f7a0e9e5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 1. Load original model\n",
    "model_id = \"./nairs-2d\"\n",
    "peft = PeftConfig.from_pretrained(model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_id).eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
