{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975517a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, TaskType, PeftModel\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "callbacks = EarlyStoppingCallback(early_stopping_patience=3,\n",
    "                                 early_stopping_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce081b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd56b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"json\", data_files=\"Data/Physics_questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc199e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9aa401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': \"When soldiers march in step across a suspension bridge, their rhythmic footsteps can create a dangerous phenomenon. Each footstep produces a small force that can cause the bridge to vibrate. If these vibrations match the bridge's natural frequency, the amplitude of oscillations can increase dramatically.\",\n",
       " 'question': 'Marching soldiers crossing a suspension bridge are usually advised to break their steps to avoid damaging the bridge owing to',\n",
       " 'options': {'A': 'oscillation',\n",
       "  'B': 'resonance',\n",
       "  'C': 'swinging',\n",
       "  'D': 'vibration'},\n",
       " 'correct_option': 'B',\n",
       " 'explanation': 'Marching soldiers crossing a suspension bridge are usually advised to break their steps to avoid damaging the bridge owing to resonance. The steps of the marching soldiers can set the bridge into vibration, and when the frequency of the bridge is equal to that of the steps of the soldiers, the resonance occurs, and at this resonance, the bridge vibrates violently with maximum amplitude, and can collapse.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4747aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: flash_attention_2\n",
      "[INFO] Using model_id: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9099d4d87b474401aa7f554521b5d142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "#device_map = {\"\": 0}\n",
    "use_quantization_config = False\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config = True \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llama = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                              quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                               low_cpu_mem_usage=True,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                attn_implementation=attn_implementation\n",
    "                                           )\n",
    "if not use_quantization_config:\n",
    "    llama.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55f411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ec7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcq(example):\n",
    "    input_text = f\"\"\"Context: {example['input_text']}\n",
    "\n",
    "Question: {example['question']}\n",
    "\n",
    "Options:\n",
    "A: {example['options']['A']}\n",
    "B: {example['options']['B']}\n",
    "C: {example['options']['C']}\n",
    "D: {example['options']['D']}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    target = f\"{example['correct_option']}\"\n",
    "    explanation = example.get('explanation')\n",
    "    if explanation:\n",
    "        target = f\"Answer: {example['correct_option']}\\nExplanation: {explanation}\"\n",
    "\n",
    "    return {'input': input_text, 'target': target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb800aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = [format_mcq(ex) for ex in df[\"train\"]]\n",
    "dataset = Dataset.from_list(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c12cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02484ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b105087935e47579e78392009eed131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    model_input = tokenizer(example[\"input\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    label = tokenizer(example[\"target\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_input[\"labels\"] = label[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79300a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7418e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "lora_conf =  LoraConfig(task_type=\"CAUSAL_LM\",\n",
    "                       r=64,\n",
    "                       lora_alpha=16,\n",
    "                       lora_dropout=0.1,\n",
    "                        bias='none',\n",
    "                       target_modules=[\"q_proj\", \"v_proj\"]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a67c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.4955\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(llama, lora_conf)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dabfe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab197c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd670e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training = TrainingArguments(\n",
    "    output_dir=\"./nairs-sample-4\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=8,\n",
    "    #dataset_text_field= \"input\",\n",
    "    fp16=True,  # or True, depending on your needs\n",
    "    bf16=False,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    #max_seq_length=1042,\n",
    "    #eval_strategy=\"no\",\n",
    "    eval_steps=312,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir=\"./Epochs_5\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    #load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ed624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113a827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 774\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7afd9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'labels']\n"
     ]
    }
   ],
   "source": [
    "tokenized_columns = ['input_ids', 'labels']\n",
    "tokenized_datasets = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in tokenized_columns])\n",
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "790df166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=llama,\n",
    "    args=training,\n",
    "    peft_config= lora_conf,\n",
    "    tokenizer = tokenizer,\n",
    "    max_seq_length=1042,\n",
    "    dataset_text_field=\"text\",\n",
    "    train_dataset= tokenized_dataset\n",
    "    #eval_dataset=test,\n",
    "    #callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f01314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1552' max='1552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1552/1552 1:06:42, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>0.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>0.588900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.529800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>0.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1358</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>0.446400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1552, training_loss=0.611739207788841, metrics={'train_runtime': 4006.5547, 'train_samples_per_second': 1.545, 'train_steps_per_second': 0.387, 'total_flos': 6.316099693549978e+16, 'train_loss': 0.611739207788841, 'epoch': 8.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e18127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./nairs-2e\")\n",
    "# tokenizer.save_pretrained(\"./nairs-2e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb28f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cfa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================\n",
    "# Loading fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6599615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_map = {\"\": 0}\n",
    "model_id = \"./nairs-2e\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config = True \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llama = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                             torch_dtype = torch.float16,\n",
    "                                              quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                               low_cpu_mem_usage=True,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                attn_implementation=attn_implementation\n",
    "                                           )\n",
    "if not use_quantization_config:\n",
    "    llama.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c77a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/shegun93/anaconda3/envs/n_project/lib/python3.9/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment: What is the principle of relativity?\n",
      "\n",
      "The fundamental principles of relativity, as established by Albert Einstein, are:\n",
      "\n",
      "1. The laws of physics are the same for all observers in uniform motion relative to one another.\n",
      "2. The speed of light in a vacuum is constant and unchanging for all observers, regardless of their relative motion.\n",
      "\n",
      "These principles challenge the long-held belief that time and space are absolute. They form the foundation of modern physics and have far-reaching implications for our understanding of space, time, and gravity.\n",
      "\n",
      "What are the two fundamental principles of relativity?\n",
      "\n",
      "Options:\n",
      "A: The laws of physics are relative to the observer.\n",
      "B: The speed of light is relative to the observer.\n",
      "C: Time and space are absolute.\n",
      "D: The laws of physics are different for different observers.\n",
      "\n",
      "Answer: B: The speed of light is relative to the observer.\n",
      "\n",
      "Explanation: The fundamental principles of relativity are the foundation of modern physics. They challenge the belief that time and space are absolute and establish that the laws of physics are the same for all observers in uniform motion relative to one another. The speed of light is also constant and\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the principle of relativity\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = nairs.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens= 256,\n",
    "        temperature= 0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Assessment:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f7012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b1cc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_physics_assessment(naira, tokenizer, context, max_new_tokens=600, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generates properly formatted physics assessments with guaranteed structure.\n",
    "    Implements multiple fallback mechanisms for reliable output.\n",
    "    \"\"\"\n",
    "    # 1. Create an explicit few-shot prompt with clear formatting examples\n",
    "    prompt = f\"\"\"Generate an assessment question with options and provide a detailed explanation using EXACTLY this format:\n",
    "\n",
    "Example 1:\n",
    "Context: When soldiers march across a suspension bridge...\n",
    "Question: Why are marching soldiers advised to break step on bridges?\n",
    "Options:\n",
    "A: To reduce air resistance\n",
    "B: To prevent resonance\n",
    "C: To minimize friction\n",
    "D: To decrease bridge weight\n",
    "Answer: B\n",
    "Explanation: Marching soldiers are advised to break step on bridges to prevent resonance. When soldiers march in unison, their rhythmic footsteps can match the bridge's natural frequency. This matching of frequencies can cause the bridge to oscillate with increasing amplitude, potentially leading to structural damage. Breaking step ensures that the periodic force isn't applied at the bridge's natural frequency, preventing dangerous resonance effects.\n",
    "\n",
    "Now generate for:\n",
    "Context: {context}\n",
    "Question:\"\"\"\n",
    "\n",
    "    # 2. Generate the output with conservative parameters\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = llama.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # 3. Extract and clean the generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = full_output.split(\"Question:\")[-1].strip()\n",
    "    return generated_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7bdddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which relative motion is a paradox?\n",
      "Options:\n",
      "A: Two trains moving at 30 km/h are at relative rest\n",
      "B: Two cars traveling at 100 km/h move past each other\n",
      "C: Two satellites orbiting Earth\n",
      "D: Two particles colliding\n",
      "Answer: C\n",
      "Explanation: According to the Special Theory of Relativity, when two objects move at different relative velocities, their space and time coordinates are relative to each other. When two satellites orbit Earth, they are in relative motion, but they are not in relative motion with respect to space. This is a paradox because both are moving at constant velocity, and their motion is independent of the other's.\n",
      "\n",
      "Please provide your question.\n"
     ]
    }
   ],
   "source": [
    "context = \"What is Principle of Relativity\"\n",
    "assessment = generate_physics_assessment(nairs, tokenizer, context)\n",
    "print(assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e90acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa0dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faf02955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./nairs-2d/tokenizer_config.json',\n",
       " './nairs-2d/special_tokens_map.json',\n",
       " './nairs-2d/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./nairs-2d\")\n",
    "tokenizer.save_pretrained(\"./nairs-2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c79f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
